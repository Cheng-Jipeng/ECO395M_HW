---
title: "Exercise 4"
author: "Yuting Huang, Jipeng Cheng, Weidi Hou"
date: "5/1/2022"
output: bookdown::html_document2
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo=FALSE)
```
# Q1: Clustering and PCA
## Clustering
### Color of wines
```{r fig1, fig.align='center', fig.cap="Value of features for wines of different colors", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
library(RCurl)
library(tidyverse)
library(mosaic)
library(LICORS)  # for kmeans++
library(ggplot2)
library(ggpubr)
library(reshape2) # stack data by PC

# Read data #
wine_original = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/wine.csv")
wine = wine_original %>% 
  select(-quality,-color) %>% 
  scale(center=TRUE, scale=TRUE) # standardize data


# Clustering #
## Kmean++
wine_clust_color = kmeanspp(wine, k = 2, nstart = 25)
#unique(wine_original$quality)
wine_clust_quality = kmeanspp(wine, k = 7, nstart = 25)
wine_results = data.frame(wine, color = wine_original$color, 
                          quality = wine_original$quality,
                          clust_color = wine_clust_color$cluster,
                          clust_quality = wine_clust_quality$cluster)


# Clustering Assessment #
## Color ##
### Data visualization
wine_plot_data_clust = cbind(wine_results[12:15], stack(wine_results[1:11]))
ggplot(wine_plot_data_clust) +
  geom_boxplot(aes(x=color, y=values)) +
  facet_wrap(~ind, ncol = 3) +
  labs(y = "Value of Chemicals",
       x="Actual Color")# mostly differed by va and tsd
```

Figure \@ref(fig:fig1) shows that colors can be most distinguished by `volatile.acidity` and `total.sulfur.dioxide` since these two features' differences in median are significantly large. After running K-means++, we can find that `volatile.acidity` and `total.sulfur.dioxide` naturally help divide the wines into 2 groups by Figure \@ref(fig:fig2).

```{r fig2, fig.align='center', fig.cap="Features by clusters (color)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
### Labels emerge naturally from clustering
# refernce: 【7.1】cars.R
ggplot(wine_results) + 
  geom_point(aes(x=volatile.acidity, y=total.sulfur.dioxide,
                 col=factor(clust_color))) +
  labs(color='Generated Cluster') 
```

This can be confirmed by plotting the true color of wines (Figure \@ref(fig:fig3)), which shows group 1 refers to the whites and group 2 refers to the reds.

```{r fig3, fig.align='center', fig.cap="Features by actual labels (color)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
ggplot(wine_results) + 
  geom_point(aes(x=volatile.acidity, y=total.sulfur.dioxide,
                 col=factor(color))) +
  labs(color='Actual Color')
```

The confusion matrix gives the precision of using K-means++ to differ the reds from the whites.

```{r warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
### Compare cluster and labels
confusion_color = table(y=wine_results$color, yhat=wine_results$clust_color)
confusion_color[2:1,]
```

### Quality of wines
Figure \@ref(fig:fig4) shows that it can be hard to figure out significant chemical differences among wines of different quality. 

```{r fig4, fig.align='center', fig.cap="Value of features for wines of different quality", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
## Quality ##
### Data visualization
ggplot(wine_plot_data_clust) +
  geom_boxplot(aes(x=factor(quality), y=values)) +
  facet_wrap(~ind, ncol = 3) +
  labs(y = "Value of Chemicals", x="Actual Quality")
```

However, if we focus on the feature's median values for wines of different quality as in Figure \@ref(fig:fig5), we may expect that at least `density` and `alcohol` can distinguish wines of high quality from the rest, and `volatile.acidity` and `free.sulfur.dioxide` can differ wines of low quality from others. 

```{r fig5, fig.align='center', fig.cap="Median value of features for wines of different quality", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
#wine_plot_data_clust %>%
#  group_by(quality, ind) %>%
#  summarise(mean_feature = mean(values)) %>%
#  ggplot() +
#  geom_point(aes(x=factor(quality), y=mean_feature)) +
#  facet_wrap(~ind, ncol=3) 
wine_plot_data_clust %>%
  group_by(quality, ind) %>%
  summarise(median_feature = median(values)) %>%
  ggplot() +
  geom_point(aes(x=factor(quality), y=median_feature)) +
  facet_wrap(~ind, ncol = 3) +
  labs(y = "Median Value of Chemicals",
       x="Actual Quality")# mostly differed by chlo and dens, ca and fsd
```

After running K-means++ to divide wines into 7 groups (since the actual quality of wines are only ranged from 3 to 9), we do see the some differences in quality emerge naturally from clustering by Figure \@ref(fig:fig6). 

```{r fig6, fig.align='center', fig.cap="Features by clusters (quality)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
### Labels emerge naturally from clustering
#### clustering labels 
clust_1 = ggplot(wine_results) + 
  geom_point(aes(x=density, y=alcohol,
                 col=factor(clust_quality))) +
  labs(color="Generated Cluster")
clust_2 = ggplot(wine_results) + 
  geom_point(aes(x=volatile.acidity, y=free.sulfur.dioxide,
                 col=factor(clust_quality))) +
  labs(color="Generated Cluster")
clust_3 = ggplot(wine_results) + 
  geom_point(aes(x=density, y=free.sulfur.dioxide,
                 col=factor(clust_quality))) +
  labs(color="Generated Cluster")
clust_4 = ggplot(wine_results) + 
  geom_point(aes(x=alcohol, y=free.sulfur.dioxide,
                 col=factor(clust_quality))) +
  labs(color="Generated Cluster")
ggarrange(clust_1, clust_2, clust_3, clust_4, 
          ncol = 2, nrow=2, common.legend = TRUE,
          legend = "right") 
```

However, if we care about the precision of the clustering, the true differences among wines are not accurately captured. Especially, we cannot match the generated clustered with the true groups as we did for color clustering, if we try to compare Figure \@ref(fig:fig6) and Figure \@ref(fig:fig7). This suggests that the actual quality clusters of points are not convex.

```{r fig7, fig.align='center', fig.cap="Features by actual labels (quality)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
#### orginal label
clust_11 = ggplot(wine_results) + 
  geom_point(aes(x=density, y=alcohol,
                 col=factor(quality))) +
  labs(color="Actual Quality")
clust_22 = ggplot(wine_results) + 
  geom_point(aes(x=volatile.acidity, y=free.sulfur.dioxide,
                 col=factor(quality)))  +
  labs(color="Actual Quality")
clust_33 = ggplot(wine_results) + 
  geom_point(aes(x=density, y=free.sulfur.dioxide,
                 col=factor(quality)))  +
  labs(color="Actual Quality")
clust_44 = ggplot(wine_results) + 
  geom_point(aes(x=alcohol, y=free.sulfur.dioxide,
                 col=factor(quality))) +
  labs(color="Actual Quality")
ggarrange(clust_11, clust_22, clust_33, clust_44, 
          ncol = 2, nrow= 2, common.legend = TRUE,
          legend = "right") 
```

The failure of quality clustering can be confirmed again by comparing the actual number of each group and the number of wines in each predicted clusters. If the clusters are capable of classifying the seven levels quality, then the distribution of wine numbers among clusters should look similar to the one among true quality groups. Figure \@ref(fig:fig8) shows the distribution are different and thus clustering algorithms cannot precisely distinguish higher from the lower quality wines.

```{r fig8, fig.align='center', fig.cap="Counts of each cluster / quality groups", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
### distribution 
clust_5 = wine_results %>%
  group_by(quality) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = reorder(factor(quality),(-count)), y = count)) + 
  geom_bar(stat = 'identity') +
  xlab("Actual Quality") +
  ylab("Count")+
  ylim(0,3000)
clust_6 = wine_results %>%
  group_by(clust_quality) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = reorder(factor(clust_quality),(-count)), y = count)) + 
  geom_bar(stat = 'identity') +
  xlab("Predicted Clusters") +
  ylim(0,3000) 
ggarrange(clust_5, clust_6+rremove("ylab"), ncol = 2, nrow = 1) 
```

## Principal component analysis
### Color of wines
Next, we consider PCA to see if there are some principal components that can distinguish the reds from the whites, or distinguish wines of different quality. Before that, we would like to take a look at the heatmap (Figure \@ref(fig:fig9)), which shows that there exist some correlated features that might be able to be summarized by a principal component, like `residual.sugar`, `total.sulfur.dioxide`, and `free.sulfur.dioxide`.

```{r fig9, fig.align='center', fig.cap="Heatmaps of features", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
# Principal Component Analysis #
ggcorrplot::ggcorrplot(cor(wine), hc.order = TRUE)
```

The results of PCA show that the first two components can account for about half of the variations. 

```{r warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
wine_PCA = prcomp(wine, rank = 11)
summary(wine_PCA)
wine_loadings = wine_PCA$rotation %>%
  as.data.frame %>%
  rownames_to_column('features')
wine_scores = wine_PCA$x %>%
  as.data.frame() %>%
  rownames_to_column('wine_code')
wine_results = wine_results %>% rownames_to_column('wine_code')
## color ##
wine_results = merge(wine_results, wine_scores, by = 'wine_code') 
wine_plot_data_pca = melt(wine_results, id.var = colnames(wine_results)[1:16],
                          variable.name = 'PC')
```

The preliminary visualization in Figure \@ref(fig:fig10) has indicated the significant differences in principal component 1 between red wines and white wines.

```{r fig10, fig.align='center', fig.cap="Value of principal components for wines of different colors", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
ggplot(wine_plot_data_pca) +
  geom_boxplot(aes(x=color, y=value)) +
  facet_wrap(~PC) +
  labs(y = "Value of Principal Components",
       x="Actual Color")
```

Figure \@ref(fig:fig11) further confirmed that we can distinguish the reds from the whites using principal component 1: `PC1`'s scores tend to  higher on the whites than on the reds.

```{r fig11, fig.align='center',fig.cap="Principal components by actual labels (color)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
### Groups emerge naturally from PCA
ggplot(wine_results) +
  geom_point(aes(x=PC1, y=PC2, color=factor(color))) +
  labs(y = "Principal Component 2", x="Principal Component 1",
       color='Acutal Color') 
```

### Quality of wines
Figure \@ref(fig:fig12) shows that there is no principal components that can significantly differ wines of different quality.

```{r fig12, fig.align='center', fig.cap="Value of principal components for wines of different quality", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
## quality ##
### Distributions of scores over quality
# Should look at the scores, i.e. go wine by wine or quality by quality
# what the first summary of these scores over quality?
# refernce: 【8.4】ercot_PCA.R
ggplot(wine_plot_data_pca) +
  geom_boxplot(aes(x=factor(quality), y=value)) +
  facet_wrap(~PC) +
  labs(y = "Value of Principal Components",
       x="Actual Quality")
```

However, Figure \@ref(fig:fig13) shows that principal component 2 and principal component 3 would decrease with quality on average.

```{r fig13, fig.align='center', fig.cap="Median value of principal components for wines of different colors", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
wine_plot_data_pca %>%
  group_by(quality, PC) %>%
  summarise(median_feature = median(value)) %>%
  ggplot() +
  geom_point(aes(x=factor(quality), y=median_feature)) +
  facet_wrap(~PC, ncol=3)+
  labs(y = "Median Value of Principal Components",
       x="Actual Quality") # most differed by PC2 and PC3
```

Figure \@ref(fig:fig14) shows that wines of high quality tend to have less principal component 2 and vice versa. But the pattern is not very clear and may not able to classify quality very well.

```{r fig14, fig.align='center', fig.cap="Principal components by actual labels (quality)", warning=FALSE, echo=FALSE, message=FALSE, alert=FALSE}
ggplot(wine_results) +
  geom_point(aes(x=PC2, y=PC3, color=factor(quality))) +
  labs(y = "Principal Component 3", x="Principal Component 2",
       color='Acutal Quality') 
```

# Question 2: Market segmentationent

In this questions, our task is to design a model in order to help separate people into different market segments. As we all know, sellers always care about whether they can get in touch with their target customers, and post their advertisement to people who really care about their products. correctly reaching out their target customers paves an important way for them to make decision efficiently and therefore makes more money. In order to give our client some insight as to how they might position their brand to maximally appeal to each market segment, we use the PCA (principle component analysis) method to give a incise look after analyzing the data. 

## Upload data 

```{r echo=FALSE, message=FALSE, warning=FALSE}
## Now the data of social marketing
social_marketing=read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/social_marketing.csv", row.names=1)
## head(social_marketing)
```

## PCA method

From the statement in question 2, we decide to delete category " chatter", since a lot of annotators may used the "chatter" category to capture posts that don't fit at all into any of the listed interest categories. Also, we will delete categories " spam", "adult" and "unrecognized" when we use the PCA method. Before doing PCA, the variables should be centered and scaled. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
PCA=prcomp(social_marketing-social_marketing$chatter-social_marketing$uncategorized-social_marketing$spam-social_marketing$adult, scale=TRUE)
PVE=PCA$sdev^2/sum(PCA$sdev^2)
plot(PVE,ylab="PVE", type="o", xlab="Principal Component", main="Figure 2.1 PVE (Scree Plot)")
plot(cumsum(PVE), type="o", ylab="Cumulative PVE", xlab="Principal Component", main="Figure 2.2. Cumulative PVE", col="red")
summary(PCA)
```
From Figure 2.2 Cumulative PVE and the summery result, we found that the first eight principle components can explain more than 95% variance of the data, therefore, we decide to choose eight principle components to analysis data. In addition, form Cumulative PVE, we found that after about ten principle components, the curve incline to be flat, which means after first ten principle components, extra components can't help to explain more variance.

## Result analysis

```{r echo=FALSE, message=FALSE, warning=FALSE}
round(PCA$rotation[,1:8],2)
```

From the results, we can't find large difference in the first principle components, so it's not helpful for us to decide people appealing to any market segment. But in the fourth principle component, we found that categories "cooking", "photo_sharing", "fashion",  have higher values. Based on these characteristics, we think they may be urban with-collar worker, since these are all their interested topics. Then from the fifth principle component, we found that  "college_uni","online gaming", "health_nutrition" have higher value, they may be college students since these are all their favorite topics. In addition, from the sixth principle component, "politics", "travel", "college_uni"have the highest values, we guess they are the middle-aged, since these people like talking about politics and they have enough money and extra time to travel, also they care about the education condition of their kids. Then from the seventh principle component, categories "tv_film" and "art" have higher values, so they maybe artists. Finally, from the eighth principle component, "photo sharing" and "shopping" have higher values, we believe they maybe young female since these topics fits the characteristics of this group of people. Based on our analysis and suggestions, our client can post their advertisement to the market segments who are really interested about their brand.

# Q3: Association rules for grocery purchases

```{r, include=FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(arules) # has a big ecosystem of packages built around it.
library(arulesViz)
library(igraph)

groceries = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/groceries.txt",header = FALSE)

groceries$buyer = seq.int(nrow(groceries))
groceries = cbind(groceries[,5], stack(lapply(groceries[,1:4], as.character)))[1:2]
colnames(groceries) = c("buyer","item")
groceries = groceries[order(groceries$buyer),]
groceries = groceries[!(groceries$item==""),]
row.names(groceries) = 1:nrow(groceries)
groceries$buyer = factor(groceries$buyer)
```

In order to obtain the association rules for grocery purchases, here we used the calculation of the Apriori method with the conditions of support=0.005, confidence=0.1 and maxlen=2.

```{r, include=FALSE,message=FALSE,warning=FALSE}
groceries = split(x=groceries$item, f=groceries$buyer)
groceries = lapply(groceries, unique)
grotrans = as(groceries, "transactions")
goodrules = apriori(grotrans, 
                    parameter=list(support=.005, confidence=.1, maxlen=2))
inspect(goodrules)
inspect(subset(goodrules, lift > 10 & confidence > 0.05))
```

```{r,echo=FALSE,message=FALSE,warning=FALSE}
plot(goodrules, measure = c("support", "lift"), shading = "confidence")
```

However, it is hard to find the association rules because of so many rules in above plot. Therefore, we would like to use the subset data to make the graph clearly.

```{r, include=FALSE,message=FALSE,warning=FALSE}
sub1 = subset(goodrules, subset=confidence > 0.01 & support > 0.005)
```

We selected the association rules to form a subset with confidence \> 0.01, support \> 0.005 and 45 rules.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
plot(head(sub1, 45, by='lift'), method='graph')
```

Finally, the graph looks more interesting, reasonable and meaningful. For instance, whipped/sour cream, cream, cheese, and butter point to yogurt, which are possible because they belongs to dairy product. In addition, meat, fruit and vegetables close to other vegetables.