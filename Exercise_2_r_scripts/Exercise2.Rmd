---
title: "Exercise 2"
author: "Yuting Huang, Jipeng Cheng, Weidi Hou"
date: "3/5/2022"
output:
<<<<<<< Updated upstream
  word_document: default
=======
>>>>>>> Stashed changes
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

# Problem 1: visualization
## 1.1 Plot 1: average boarding line graph

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(RCurl)
library(mosaic)
library(ggplot2)
capmetro_UT = read.csv('https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/capmetro_UT.csv')

capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                     levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month, levels=c("Sep", "Oct","Nov")))

avg_boarding = capmetro_UT %>%
  group_by(hour_of_day, day_of_week, month) %>%
  summarize(meanboarding = mean(boarding)) 

week = mutate(capmetro_UT, weekdays = day_of_week == "Mon", "Tue", "Wed","Thu", "Fri", weekend = day_of_week == "Sat", "Sun")

ggplot(avg_boarding) +
  geom_line(aes(x = hour_of_day, y=meanboarding, color=month)) +
  facet_wrap(~day_of_week) +
  labs(title="Average boardings grouped by hour of the day, day of week, and month", y="mean boarding", x="hour of day")
```

According to the graph, it shows that the hour of peak boardings do not change from day to day, and it broadly similar showing the peak hour at 15-17.

**Does the hour of peak boarding change from day to day, or is it broadly similar across days?**

According to the graph, it shows that the hour of peak boardings do not change from day to day, and it broadly similar showing the peak hour at 15-17.

**Why do you think average boarding on Mondays in September look lower, compared to other days and months?** 

We can guess that there are less average boardings on September because the beginning of Fall semester. On the other hand, students may not prefer choosing the courses on Monday due to "Monday Blue".

**Similarly, why do you think average boardings on Weds/Thurs/Fri in November look lower?**

Average boarding on Weds/Thurs/Fri in November look lower because students may have to prepare for the midterm exam, they would like to stay home rather than go outside.

## 1.2 Plot 2: scatter plots showing boardings vs. temperature

```{r message=FALSE, echo=FALSE, warning=FALSE}

avg_boarding_tem = capmetro_UT %>%
  group_by(temperature, hour_of_day,  weekend) %>%
  summarize(meanboarding = mean(boarding)) 

ggplot(avg_boarding_tem) +
  geom_point(aes(x = temperature, y=meanboarding, color=weekend)) +
  facet_wrap(~hour_of_day) +
  labs(title="Average boardings grouped by temperature and week", y="mean boarding", x="temperature")
```

**When we hold hour of day and weekend status constant, does temperature seem to have a noticeable effect on the number of UT students riding the bus?**

According to above graph, temperature seem to have no noticeable effect on the number of UT students riding the bus.

# Problem 2: Saratoga house prices

## 2.1 The best linear model
```{r echo=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(parallel)
library(foreach)
library(ggpubr)
library(kknn)
library(kableExtra)
data(SaratogaHouses)

# Split into training and testing sets with K folds
K_folds = 5
sh_folds = crossv_kfold(SaratogaHouses, k = K_folds)
# Question 1
lm1 = map(sh_folds$train, ~ lm(price ~ lotSize + bedrooms + bathrooms, data=. ))
lm2 = map(sh_folds$train, ~ lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction,
                               data=. ))
lm3 = map(sh_folds$train, ~ lm(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)^2,
                               data=. ))
lm4 = map(sh_folds$train, ~ lm(price ~ livingArea + centralAir + bathrooms + fuel + 
                                 lotSize + bedrooms + rooms + livingArea:centralAir + livingArea:bathrooms + 
                                 livingArea:fuel + livingArea:rooms + bathrooms:bedrooms + centralAir:fuel + 
                                 bathrooms:fuel + fuel:lotSize + centralAir:bathrooms +  bedrooms:rooms, data=. ))

errs_lm1 = map2_dbl(lm1, sh_folds$test, modelr::rmse)
errs_lm2 = map2_dbl(lm2, sh_folds$test, modelr::rmse)
errs_lm3 = map2_dbl(lm3, sh_folds$test, modelr::rmse)
errs_lm4 = map2_dbl(lm4, sh_folds$test, modelr::rmse)
# Predictions out of sample
# Root Mean squared error
c(errs_lm1 = mean(errs_lm1), errs_lm2= mean(errs_lm2), errs_lm3 = mean(errs_lm3), errs_lm4 =  mean(errs_lm4))
```

**Build the best linear model for price that you can. It should clearly outperform the "medium" model that we considered in class. Use any combination of transformations, engineering features, polynomial terms, and interactions that you want; and use any strategy for selecting the model that you want.**

The result suggests that lm4 is the best, since it has the lowest cross validation rmse. The formular of linear model 4 is as below. The cross-validation rmse of lm4 is smaller than that of medium model. Therefore, our model overperfom the medium model.

$price=livingarea+centralair+bathrooms+fuel+lotsize+bedrooms+rooms+livingarea\times centralair+livingarea \times bathrooms+livingarea \times fuel+livingarea \times rooms+bathrooms \times bedrooms+centralair \times fuel+bathroom \times fuel+fuel \times lotsize+centralair \times bathrooms+bedrooms \times rooms  $


## 2.2 The Best KNN

We will find optimal k and features simultaneously.

```{r message=FALSE, warning=FALSE, echo=FALSE}

## Features from lm1

k_grid = rep(1:125)
cv_sh_knn1 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ lotSize + bedrooms + bathrooms,
                                        data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh1 = cv_sh_knn1 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn1_err = cv_sh_knn1 %>% filter(k==k_min_rmse_sh1)
## Features from lm2 & lm3
cv_sh_knn2 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)
                                        , data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh2 = cv_sh_knn2 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn2_err = cv_sh_knn2 %>% filter(k==k_min_rmse_sh2)
## Features from lm4
cv_sh_knn3 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ livingArea + centralAir + bathrooms + bedrooms + 
                                          lotSize + fuel + rooms
                                        , data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh3= cv_sh_knn3 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn3_err = cv_sh_knn3 %>% filter(k==k_min_rmse_sh3)
## Cross-validation OOS Performance Errors for KNN
rbind(knn1 = cv_sh_knn1_err,knn2 = cv_sh_knn2_err, knn3 = cv_sh_knn3_err)
## To .Rmd writer: You may write some codes to incorporate these 2 results since we are required to compared KNN results
## with linear regression results.

# XDD

```

**Which model seems to do better at achieving lower out-of-sample mean-squared error?**

Analysis: According to our results, the cross-validation error is lower for linear model, so linear model seems to do better at achieving lower out of sample mean squared error. However, the best variables and cross-validation error for knn and linear model is pretty similar, the difference is $68217.62-64837.09=3380.53$, which is very small compared to their value.

# problem 3: Classification and retrospective sampling
## 3.1 bar plot of default probability by credit history
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(RCurl)
library(tidyverse)

g_c = read.csv('https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/german_credit.csv')

##g_c %>%
  ##group_by(history) %>%
  ##count(Default==1)

ggplot(aes(x=Default/300, y=history), data=g_c) +
  geom_col() +
  labs(title="default probability by credit history", y="history performance", x="Default probability")
```

## 3.2 logistic regression model
```{r message=FALSE, warning=FALSE, echo=TRUE}
g_c_glm = glm(formula = Default ~ duration + amount + installment + age + 
      history + purpose + foreign, family = "binomial", data = g_c)
```
**What do you notice about the history variable vis-a-vis predicting defaults? What do you think is going on here? In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default? Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?**

Based on the data, the default probability of people with good history is higher than the default probability of people with poor history. There is a sampling problem in this model, since this model based on the "case-control" design. To be specific, if we want to choose three features as factors in the model, but only two of them have been chosen in to the model but one feature has been omitted, it will lead to more incorrect analysis for those two features. It's a type of problem of selection bias and misrepresentation. Based on our previous analysis, this data set is inappropriate for building a predictive model of defaults. We recommend the bank change their sampling scheme in order to solve the unbalanced sample problem, such as using random sampling method. 

# problem 4: Children and hotel reservations
## 4.1 build model
Count the dependent variable:
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(RCurl)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(gamlr)
library(rsample)
library(modelr)
library(parallel)
library(foreach)

# Read in data
hotel_dev = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/hotels_dev.csv")
hotel_val = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/hotels_val.csv")
hotel_dev = hotel_dev %>% filter(reserved_room_type != "L") 
#// Because of no corresponding data points in hotel_val.csv, see DEBUG at the end for more details.
# Data browsing
ggplot(data = hotel_dev) +
  geom_histogram(aes(x=children), binwidth=0.5)
hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)
```

### 4.1.1 Build baseline model
```{r warning=FALSE, message=FALSE}
baseline1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest,
                data = hotel_dev_train, family = "binomial")
baseline2 = glm(children ~ . - arrival_date , data = hotel_dev_train, family = "binomial")
```

### 4.2 Build best model - Feature engineering with LASSO 
idea: use LASSO to find main effects + interaction by eyeballing
```{r warning=FALSE, message=FALSE}
hotel_lasso_x_main = model.matrix(children ~  (.-1-arrival_date), data=hotel_dev_train)
hotel_lasso_x_itac = model.matrix(children ~  (.-1-arrival_date)^2, data=hotel_dev_train)
hotel_lasso_y = hotel_dev_train$children
#//see https://cran.r-project.org/web/packages/gamlr/gamlr.pdf to see more
hotel_lasso_main = cv.gamlr(hotel_lasso_x_main, hotel_lasso_y, nfold=10, verb=TRUE, family="binomial")
hotel_lasso_itac = cv.gamlr(hotel_lasso_x_itac, hotel_lasso_y, nfold=10, verb=TRUE, family="binomial")
```

**Extract strong single covariates:**
```{r warning=FALSE, message=FALSE, echo=FALSE}
coef(hotel_lasso_main, select='min')
```
From the output, we see that the variable **deposit_type** is insignificant from zero. Therefore, we rule out it.

**Extract strong interactions:**
```{r warning=FALSE, message=FALSE, echo=FALSE}
strong_interaction_name = coef(hotel_lasso_itac, select = 'min')@Dimnames[1] %>% as.data.frame() 
strong_interaction_name = strong_interaction_name[coef(hotel_lasso_itac, select = 'min')@i,] 
strong_interaction_beta = coef(hotel_lasso_itac, select = 'min')@x[-1]
coef_lasso = cbind(strong_interaction_name, strong_interaction_beta) %>% # transform matrix to dataframe
  as.data.frame() %>%
  mutate(abs_beta = abs(as.numeric(strong_interaction_beta))) 
coef_lasso %>% # filter in strong interaction
  filter(!(strong_interaction_name %in% colnames(hotel_dev))) %>%
  arrange(desc(abs_beta)) %>%
  head(30) 
#// pick (meal:reserved_room_type, reserved_room_type:assigned_room_type, hotel:reserved_room_type, market_segment:reserved_room_type,
#// meal:is_repeated_guest, adults:previous_bookings_not_canceled, meal:previous_bookings_not_canceled, market_segment:customer_type,
#// is_repeated_guest:assigned_room_type, assigned_room_type:required_car_parking_spaces) by eyeballing
```
From the results, we pick these terms: $meal \times \text{reserved_room_type}$, $\text{assigned_room_type} \times \text{reserved_room_type}$, $hotel \times \text{reserved_room_type}$, $\text{market_segment} \times \text{reserved_room_type}$,$meal \times \text{is_repeated_guest}$, $adult \times \text{previous_bookings_not_canceled}$, $meal \times \text{previous_bookings_not_canceled}$, $\text{market_segment}\times  \times \text{customer_type}$, $\text{is_repeated_guest} \times \text{assigned_room_type}$, $\text{assigned_room_type} \times \text{required_car_parking_spaces}$ by eyeballing, since they are significant from zero.

**Rule out non-converged covariates & interactions:**
```{r warning=FALSE, message=FALSE, echo=FALSE}
lasso_selected_try = glm(children ~ (.-arrival_date-deposit_type) + meal:reserved_room_type+ reserved_room_type:assigned_room_type+
                           hotel:reserved_room_type+ market_segment:reserved_room_type+meal:is_repeated_guest+ 
                           adults:previous_bookings_not_canceled+ meal:previous_bookings_not_canceled+ market_segment:customer_type+
                           is_repeated_guest:assigned_room_type+ assigned_room_type:required_car_parking_spaces, 
                         data = hotel_dev_train, family = "binomial")
coef(lasso_selected_try) # rule out non-converged covariates & interactions
lasso_selected = glm(children ~ (.-arrival_date-deposit_type) + hotel:reserved_room_type+ meal:is_repeated_guest+ 
                           adults:previous_bookings_not_canceled+ meal:previous_bookings_not_canceled+ market_segment:customer_type+
                           is_repeated_guest:assigned_room_type+ assigned_room_type:required_car_parking_spaces, 
                         data = hotel_dev_train, family = "binomial")
```
We rule out all the variables and interactions with **NA**, since they are non-converged.

## 4.3 Out-of-sample performance evaluation
```{r echo=FALSE, message=FALSE, warning=FALSE}
### baseline evaluation
#### calculate deviance
test_child_index = which(hotel_dev_test$children == 1) # find true book with children
phat_baseline1 = predict(baseline1, hotel_dev_test, type = "response") # baseline1
baseline1_predict_deviance = -2 * sum(log(phat_baseline1[test_child_index]))
phat_baseline2 = predict(baseline2, hotel_dev_test, type = "response") # baseline2
baseline2_predict_deviance = -2 * sum(log(phat_baseline2[test_child_index]))
phat_lasso_selected = predict(lasso_selected, hotel_dev_test, type = "response") # lasso_selected
lasso_selected_predict_deviance = -2 * sum(log(phat_lasso_selected[test_child_index]))
#### confusion matrix + relevant evaluation
yhat_baseline1 = ifelse(phat_baseline1>0.5, 1, 0)
yhat_baseline2 = ifelse(phat_baseline2>0.5, 1, 0)
yhat_lasso_selected = ifelse(phat_lasso_selected>0.5, 1, 0)
confusion_baseline1 = table(y=hotel_dev_test$children, yhat=yhat_baseline1)
confusion_baseline1 = cbind(confusion_baseline1, c(0,0))
confusion_baseline2 = table(y=hotel_dev_test$children, yhat=yhat_baseline2)
confusion_lasso_selected = table(y=hotel_dev_test$children, yhat=yhat_lasso_selected)

## (4) Output: a table of measuring out-of-sample performance
measurement = c("Deviance", "TPR", "FPR", "FDR")
eval_baseline1 = c(baseline1_predict_deviance,
      confusion_baseline1[2,2]/(confusion_baseline1[2,2]+confusion_baseline1[2,1]),
      confusion_baseline1[1,2]/(confusion_baseline1[1,1]+confusion_baseline1[1,2]),
      confusion_baseline1[1,2]/(confusion_baseline1[1,2]+confusion_baseline1[2,2])) %>% round(3)
eval_baseline2 = c(baseline2_predict_deviance,
      confusion_baseline2[2,2]/(confusion_baseline2[2,2]+confusion_baseline2[2,1]),
      confusion_baseline2[1,2]/(confusion_baseline2[1,1]+confusion_baseline2[1,2]),
      confusion_baseline2[1,2]/(confusion_baseline2[1,2]+confusion_baseline2[2,2])) %>% round(3)
eval_lasso_selected = c(lasso_selected_predict_deviance,
                        confusion_lasso_selected[2,2]/(confusion_lasso_selected[2,2]+confusion_lasso_selected[2,1]),
                        confusion_lasso_selected[1,2]/(confusion_lasso_selected[1,1]+confusion_lasso_selected[1,2]),
                        confusion_lasso_selected[1,2]/(confusion_lasso_selected[1,2]+confusion_lasso_selected[2,2])) %>% round(3)
rbind(measurement, eval_baseline1, eval_baseline2, eval_lasso_selected)
```

## 4.4 Model Validation: Step 1

```{r echo=FALSE, message=FALSE, warning=FALSE}
## (1) Preidcition with validation set
phat_lasso_predict_best = predict(lasso_selected, hotel_val, type = "response")
## (2) Calculate TPR & FPR vs. t
t_grid = rep(1:49)/50
ROC_df = foreach(t = t_grid, .combine='rbind') %dopar% {
  yhat_best = ifelse(phat_lasso_predict_best > t, 1, 0)
  confusion_best = table(y=hotel_val$children, yhat=yhat_best)
  TPR_best = confusion_best[2,2]/(confusion_best[2,2]+confusion_best[2,1]) %>% round(3)
  FPR_best = confusion_best[1,2]/(confusion_best[1,1]+confusion_best[1,2]) %>% round(3)
  c(t=t, TPR = TPR_best, FPR = FPR_best)
} %>% as.data.frame()
## (3) Plot the graph
ggplot(ROC_df) +
  geom_line(aes(x=t, y=TPR, color = "TPR"), size=1) +
  geom_line(aes(x=t, y=FPR, color = "FPR"), size=1) +
  labs(y="TPR/FPR", x = "t", color=" ")
### real ROC Curve (use this)  
ggplot(ROC_df) +
  geom_line(aes(x=FPR, y=TPR), size=1) +
  labs(y="TPR", x = "FPR", color=" ")
```

## 4.5 Model Validation: Step 2
```{r echo=FALSE, message=FALSE, warning=FALSE}
K_folds = 20

hotel_val = hotel_val %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(hotel_val)) %>% sample)

hotel_val_cv = foreach(fold = 1:K_folds, .combine='rbind') %dopar% {
  hotel_val_folds_train = filter(hotel_val, fold_id == fold)
  hotel_val_folds_phat = predict(lasso_selected, hotel_val_folds_train, type = "response")
  c(y=sum(hotel_val_folds_train$children), E_y=sum(hotel_val_folds_phat)%>%round(0))
} %>% as.data.frame()

hotel_val_cv = hotel_val_cv %>%
  arrange(y) %>%
  mutate(fold_id = rep(1:K_folds))

ggplot(data = hotel_val_cv) +
  geom_line(aes(x=fold_id, y=y, color = "Actual # of children"),  size=1) +
  geom_line(aes(x=fold_id, y=E_y, color = "Predicted # of children"), size=1) +
  labs(y="Predicted / Actual Children for Each Fold", x = "t", color=" ")+
  geom_point(aes(x=fold_id, y=y)) +
  geom_point(aes(x=fold_id, y=E_y))

```

From the plot, the difference of each fold for actual data and predicted model is small, which is almost smaller than 5. Therefore, our model do well at predicting the total number of bookings with children in a group of 250 bookings.
