---
title: "Exercise 2"
author: "Yuting Huang, Jipeng Cheng, Weidi Hou"
date: "3/5/2022"
output: bookdown::html_document2
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo=FALSE)
```

# Visualization

```{r fig1, fig.align='center', fig.cap="Unchanged hour of peak boardings, lower boardings on Mondays in Spetember due to post-holiday blues, and lower boardings on Weds/Thurs/Fri in November because students tend to prepare for the midterm exam at home", echo=FALSE, message=FALSE}
library(RCurl)
library(mosaic)
library(ggplot2)
capmetro_UT = read.csv('https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/capmetro_UT.csv')

capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                     levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month, levels=c("Sep", "Oct","Nov")))

avg_boarding = capmetro_UT %>%
  group_by(hour_of_day, day_of_week, month) %>%
  summarize(meanboarding = mean(boarding)) 

week = mutate(capmetro_UT, weekdays = day_of_week == "Mon", "Tue", "Wed","Thu", "Fri", weekend = day_of_week == "Sat", "Sun")

ggplot(avg_boarding) +
  geom_line(aes(x = hour_of_day, y=meanboarding, color=month)) +
  facet_wrap(~day_of_week) +
  labs(y="Average Boardings", x="Hour of Day")
```

```{r fig2, fig.align='center', fig.cap="Unnoticeable effects of temperature on boardings", echo=FALSE, message=FALSE}

avg_boarding_tem = capmetro_UT %>%
  group_by(temperature, hour_of_day,  weekend) %>%
  summarize(meanboarding = mean(boarding)) 

ggplot(avg_boarding_tem) +
  geom_point(aes(x = temperature, y=meanboarding, color=weekend)) +
  facet_wrap(~hour_of_day) +
  labs(y="Average Boardings", x="Temperature")
```
# Saratoga house prices

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(parallel)
library(foreach)
library(ggpubr)
library(kknn)
library(kableExtra)
data(SaratogaHouses)

# K-fold train/test splits
K_folds = 5
sh_folds = crossv_kfold(SaratogaHouses, k = K_folds)
# Question 1

lm1 = map(sh_folds$train, ~ lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=.))
lm2 = map(sh_folds$train, ~ lm(price ~ lotSize + bedrooms + bathrooms, data=. ))
lm3 = map(sh_folds$train, ~ lm(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)^2, data=. ))
lm4 = map(sh_folds$train, ~ lm(price ~ livingArea + centralAir + bathrooms + fuel + lotSize + bedrooms + rooms + livingArea:centralAir + livingArea:bathrooms + livingArea:fuel + livingArea:rooms + bathrooms:bedrooms + centralAir:fuel + bathrooms:fuel + fuel:lotSize + centralAir:bathrooms +  bedrooms:rooms, data=. ))

errs_lm1 = map2_dbl(lm1, sh_folds$test, modelr::rmse)
errs_lm2 = map2_dbl(lm2, sh_folds$test, modelr::rmse)
errs_lm3 = map2_dbl(lm3, sh_folds$test, modelr::rmse)
errs_lm4 = map2_dbl(lm4, sh_folds$test, modelr::rmse)
# Predictions out of sample
# Root Mean squared error
cbind(mean(errs_lm1), mean(errs_lm2), mean(errs_lm3),mean(errs_lm4)) %>%
  knitr::kable(caption = "The RMSEs of Linear Models",
               col.names = c("Meiduam Model in Class", "New Model 1", "New Model 2", "New Model 3")) %>%
  kable_styling(full_width = F)
```
The result suggests that New Model 3 is the best linear model for price prediction, since it has the lowest cross-validation RMSE and outperforms the medium model in class. The specification of Model 3 is
$$
price=livingarea+centralair+bathrooms+fuel+lotsize+bedrooms+rooms
\\ +livingarea\times centralair+livingarea \times bathrooms+livingarea \times fuel
\\ +livingarea \times rooms+bathrooms \times bedrooms+centralair \times fuel+bathroom \times fuel
\\ +fuel \times lotsize+centralair \times bathrooms+bedrooms \times rooms
$$
```{r message=FALSE, warning=FALSE, echo=FALSE}
## Features from lm1

k_grid = rep(1:125)
cv_sh_knn1 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ lotSize + bedrooms + bathrooms,
                                        data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh1 = cv_sh_knn1 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn1_err = cv_sh_knn1 %>% filter(k==k_min_rmse_sh1)
## Features from lm2 & lm3
cv_sh_knn2 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)
                                        , data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh2 = cv_sh_knn2 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn2_err = cv_sh_knn2 %>% filter(k==k_min_rmse_sh2)
## Features from lm4
cv_sh_knn3 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sh_folds$train, ~ knnreg(price ~ livingArea + centralAir + bathrooms + bedrooms + 
                                          lotSize + fuel + rooms
                                        , data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, sh_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_min_rmse_sh3= cv_sh_knn3 %>%
  slice_min(err) %>%
  pull(k)
cv_sh_knn3_err = cv_sh_knn3 %>% filter(k==k_min_rmse_sh3)
## Cross-validation OOS Performance Errors for KNN
rbind(KNN1 = cv_sh_knn1_err, KNN2 = cv_sh_knn2_err, KNN3 = cv_sh_knn3_err) %>%
  knitr::kable(caption = "The RMSEs of KNN Models") %>%
  kable_styling(full_width = F)

## To .Rmd writer: You may write some codes to incorporate these 2 results since we are required to compared KNN results
## with linear regression results.

```

Given these 2 tables, the linear models seems to do better in achieving lower out of sample RMSE than KNN models do, where the first KNN model uses features in the medium linear models, the second KNN model uses features corresponding to linear model 1&2, and the third model uses features in the linear model 3. Given the out-of-sample performance measured by RMSE, the local taxing authority should choose **linear regression models** with focus on features such as *living areas, having central air-conditioning, fuel, lot size, the total number of rooms, and the number of bathrooms, bedrooms.* The **coefficients** on these features (and some of their interactions) describe how they contribute to the market value of a house and can be informative of predicting the expected market values of them. For example, Table \@ref(tab:tab1) reports several coefficients from regression with model 3 and the whole dataset and implies each bathroom values 54570\$. The local authority can decompose the market value of a house into the values of these features (captured by these coefficients) and design tax collection from them.

```{r tab1, message=FALSE, warning=FALSE, echo=FALSE}
lm4_pred = lm(price ~ livingArea + centralAir + bathrooms + fuel + 
     lotSize + bedrooms + rooms + livingArea:centralAir + livingArea:bathrooms + 
     livingArea:fuel + livingArea:rooms + bathrooms:bedrooms + centralAir:fuel + 
     bathrooms:fuel + fuel:lotSize + centralAir:bathrooms +  bedrooms:rooms, data= SaratogaHouses)
coef_lm4_pred = coef(lm4_pred)[2:7] %>% as.data.frame() 
colnames(coef_lm4_pred) = c("Predicted Value")
coef_lm4_pred %>%
  kbl(caption = "The Predicted Market Value of House Features (in Part)") %>%
  kable_styling() 
```

# Classification and reospective sampling
```{r fig3, fig.align='center', fig.cap="Sample Default Probability by History Performance", echo=FALSE, message=FALSE}
library(RCurl)
library(tidyverse)

g_c = read.csv('https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/german_credit.csv')

g_c2 = g_c %>%
  group_by(history) %>%
  summarize(p_default = sum(Default==1)/n())

ggplot(data=g_c2) +
  geom_col(aes(x=history, y=p_default)) +
  labs(x="History Performance", y="Default Probability")
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
g_c_glm = glm(formula = Default ~ duration + amount + installment + age + 
      history + purpose + foreign, family = "binomial", data = g_c)

b_g_c_glm = coef(g_c_glm)[6:7]%>% as.data.frame()
colnames(b_g_c_glm) = c("The partial effects on default probability")
b_g_c_glm %>% round(2) %>%
  kbl(caption = "History Variable vis-a-vis Predicting Defaults") %>%
  kable_styling(full_width = ) 
```

The regression result implies that having worse credit is expected to reduce the probability (or more specifically, the odds) of default. The bar plot suggests that the data is constructed in such a way that it contains too many data points of default with good credit history and few default points with bad credit history, which results from the retrospective, "case-control" sampling. The data in inappropriate for building a predictive model of defaults. Because in this data set "default = ture" and "history = good" always occur together, the model would wrongly regard having good credit as an indicator for default. We recommend the bank to change their sampling scheme in order to avoid the artificial selection bias, like using random sampling.

# Children and hotel reservations
## Model building
- Feature engineering with LASSO
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(RCurl)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(gamlr)
library(rsample)
library(modelr)
library(parallel)
library(foreach)
library(kableExtra)

# Read in data
hotel_dev = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/hotels_dev.csv")
hotel_val = read.csv("https://raw.githubusercontent.com/Cheng-Jipeng/ECO395M/master/data/hotels_val.csv")
# train/test split
hotel_dev_split = initial_split(hotel_dev, prop = 0.8)
hotel_dev_train = training(hotel_dev_split)
hotel_dev_test = testing(hotel_dev_split)

baseline1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest,
                data = hotel_dev_train, family = "binomial")
baseline2 = glm(children ~ . - arrival_date , data = hotel_dev_train, family = "binomial")
```

We use LASSO for feature selection by running LASSO regression and ruling out weak predictors. First, we only consider the main effects of predictors.
```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
hotel_lasso_x_main = model.matrix(children ~  (.-1-arrival_date), data=hotel_dev_train)
hotel_lasso_x_itac = model.matrix(children ~  (.-1-arrival_date)^2, data=hotel_dev_train)
hotel_lasso_y = hotel_dev_train$children
#//see https://cran.r-project.org/web/packages/gamlr/gamlr.pdf to see more
hotel_lasso_main = cv.gamlr(hotel_lasso_x_main, hotel_lasso_y, nfold=10, verb=TRUE, family="binomial")
hotel_lasso_itac = cv.gamlr(hotel_lasso_x_itac, hotel_lasso_y, nfold=10, verb=TRUE, family="binomial")
coef_hotel_lasso_main = coef(hotel_lasso_main, select='min') %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  filter(seg100 == 0)
colnames(coef_hotel_lasso_main) = c("Partial effects")
```
```{r tab2, echo = FALSE}
coef_hotel_lasso_main %>%
  knitr::kable(caption = "Weak Partial Effects") %>%
  kable_styling()
```
From Table \@ref(tab:tab2) , we see that the `deposit_type` contributes little to the prediction. Therefore, we rule out it. Then, we add interactions t0 the LASSO regression, and pick those with strong effects. 
```{r tab3, warning=FALSE, message=FALSE, echo=FALSE}
strong_interaction_name = coef(hotel_lasso_itac, select = 'min')@Dimnames[1] %>% as.data.frame() 
strong_interaction_name = strong_interaction_name[coef(hotel_lasso_itac, select = 'min')@i,] 
strong_interaction_beta = coef(hotel_lasso_itac, select = 'min')@x[-1]
coef_lasso = cbind(strong_interaction_name, strong_interaction_beta) %>% # transform matrix to dataframe
  as.data.frame() %>%
  mutate(abs_beta = abs(as.numeric(strong_interaction_beta))) 
coef_lasso = coef_lasso %>% # filter in strong interaction
  filter(!(strong_interaction_name %in% colnames(hotel_dev))) %>%
  arrange(desc(abs_beta)) %>%
  head(30) 
colnames(coef_lasso) = c("Strong Interaction","Partial Effects", "Absolute Values of Partial Effects")
coef_lasso %>%
  knitr::kable(caption = "Top 30 Strongest Predictors and Interactions") %>%
  kable_styling()
#// pick (meal:reserved_room_type, reserved_room_type:assigned_room_type, hotel:reserved_room_type, market_segment:reserved_room_type,
#// meal:is_repeated_guest, adults:previous_bookings_not_canceled, meal:previous_bookings_not_canceled, market_segment:customer_type,
#// is_repeated_guest:assigned_room_type, assigned_room_type:required_car_parking_spaces) by eyeballing
```
From Table \@ref(tab:tab3), we pick these interactions `meal:reserved_room_type `, ` reserved_room_type:assigned_room_type `, ` hotel:reserved_room_type `, ` market_segment:reserved_room_type `, ` meal:is_repeated_guest `, ` adults:previous_bookings_not_canceled `, ` meal:previous_bookings_not_canceled `, ` market_segment:customer_type `, ` is_repeated_guest:assigned_room_type `, ` assigned_room_type:required_car_parking_spaces` by eyeballing. 
```{r tab4, warning=FALSE, message=FALSE, echo=FALSE}
lasso_selected_try = glm(children ~ (.-arrival_date-deposit_type) + meal:reserved_room_type+ reserved_room_type:assigned_room_type+
                           hotel:reserved_room_type+ market_segment:reserved_room_type+meal:is_repeated_guest+ 
                           adults:previous_bookings_not_canceled+ meal:previous_bookings_not_canceled+ market_segment:customer_type+
                           is_repeated_guest:assigned_room_type+ assigned_room_type:required_car_parking_spaces, 
                         data = hotel_dev_train, family = "binomial")
coef_lasso_selected_try = coef(lasso_selected_try, select='min') %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  filter(is.na(V1))
colnames(coef_lasso_selected_try) = c("Partial effects") 
coef_lasso_selected_try %>%
  knitr::kable(caption = "Interactions That Prevent Converged LASSO") %>%
  kable_styling()
# rule out non-converged covariates & interactions
```
Finally, we rule out all the variables and interactions with `NA` according to Table \@ref(tab:tab4) since the algorithm does not converge. The selected model adds `hotel:reserved_room_type`, `meal:is_repeated_guest`, `adults:previous_bookings_not_canceled `, ` meal:previous_bookings_not_canceled `, ` market_segment:customer_type `, ` is_repeated_guest:assigned_room_type `, ` assigned_room_type:required_car_parking_spaces` as interactions.
```{r, warning=FALSE, message=FALSE, echo=FALSE}
lasso_selected = glm(children ~ (.-arrival_date-deposit_type) + hotel:reserved_room_type+ meal:is_repeated_guest+  adults:previous_bookings_not_canceled+ meal:previous_bookings_not_canceled+ market_segment:customer_type+ is_repeated_guest:assigned_room_type+ assigned_room_type:required_car_parking_spaces, 
                         data = hotel_dev_train, family = "binomial")
```
- Out-of-sample performance comparison
```{r tab5, echo=FALSE, message=FALSE, warning=FALSE}
### baseline evaluation
#### calculate deviance
test_child_index = which(hotel_dev_test$children == 1) # find true book with children
phat_baseline1 = predict(baseline1, hotel_dev_test, type = "response") # baseline1
baseline1_predict_deviance = -2 * sum(log(phat_baseline1[test_child_index]))
phat_baseline2 = predict(baseline2, hotel_dev_test, type = "response") # baseline2
baseline2_predict_deviance = -2 * sum(log(phat_baseline2[test_child_index]))
phat_lasso_selected = predict(lasso_selected, hotel_dev_test, type = "response") # lasso_selected
lasso_selected_predict_deviance = -2 * sum(log(phat_lasso_selected[test_child_index]))
#### confusion matrix + relevant evaluation
yhat_baseline1 = ifelse(phat_baseline1>0.5, 1, 0)
yhat_baseline2 = ifelse(phat_baseline2>0.5, 1, 0)
yhat_lasso_selected = ifelse(phat_lasso_selected>0.5, 1, 0)
confusion_baseline1 = table(y=hotel_dev_test$children, yhat=yhat_baseline1)
confusion_baseline1 = cbind(confusion_baseline1, c(0,0))
confusion_baseline2 = table(y=hotel_dev_test$children, yhat=yhat_baseline2)
confusion_lasso_selected = table(y=hotel_dev_test$children, yhat=yhat_lasso_selected)

## (4) Output: a table of measuring out-of-sample performance
measurement = c("Deviance", "TPR", "FPR", "FDR")
eval_baseline1 = c(baseline1_predict_deviance,
      confusion_baseline1[2,2]/(confusion_baseline1[2,2]+confusion_baseline1[2,1]),
      confusion_baseline1[1,2]/(confusion_baseline1[1,1]+confusion_baseline1[1,2]),
      confusion_baseline1[1,2]/(confusion_baseline1[1,2]+confusion_baseline1[2,2])) %>% round(3)
eval_baseline2 = c(baseline2_predict_deviance,
      confusion_baseline2[2,2]/(confusion_baseline2[2,2]+confusion_baseline2[2,1]),
      confusion_baseline2[1,2]/(confusion_baseline2[1,1]+confusion_baseline2[1,2]),
      confusion_baseline2[1,2]/(confusion_baseline2[1,2]+confusion_baseline2[2,2])) %>% round(3)
eval_lasso_selected = c(lasso_selected_predict_deviance,
                        confusion_lasso_selected[2,2]/(confusion_lasso_selected[2,2]+confusion_lasso_selected[2,1]),
                        confusion_lasso_selected[1,2]/(confusion_lasso_selected[1,1]+confusion_lasso_selected[1,2]),
                        confusion_lasso_selected[1,2]/(confusion_lasso_selected[1,2]+confusion_lasso_selected[2,2])) %>% round(3)

header.true <- function(df) {
  names(df) <- as.character(unlist(df[1,]))
  df[-1,]
}
eval_models = rbind(measurement, eval_baseline1, eval_baseline2, eval_lasso_selected) %>% as.data.frame() %>%
  header.true()
rownames(eval_models) = c("Baseline 1", "Baseline 2", "Best")
eval_models %>%
  knitr::kable(caption = "Out-of-sample Performance Evaulation") %>%
  kable_styling()
  
```
Table \@ref(tab:tab5) shows that our best model outperforms the other 2 baseline models.

## Model validation: step 1
```{r fig4, fig.align='center', fig.cap="ROC Curve", echo=FALSE, warning=FALSE, message=FALSE}
## (1) Preidcition with validation set
phat_lasso_predict_best = predict(lasso_selected, hotel_val, type = "response")
## (2) Calculate TPR & FPR vs. t
t_grid = rep(1:49)/50
ROC_df = foreach(t = t_grid, .combine='rbind') %dopar% {
  yhat_best = ifelse(phat_lasso_predict_best > t, 1, 0)
  confusion_best = table(y=hotel_val$children, yhat=yhat_best)
  TPR_best = confusion_best[2,2]/(confusion_best[2,2]+confusion_best[2,1]) %>% round(3)
  FPR_best = confusion_best[1,2]/(confusion_best[1,1]+confusion_best[1,2]) %>% round(3)
  c(t=t, TPR = TPR_best, FPR = FPR_best)
} %>% as.data.frame()
## (3) Plot the graph
### real ROC Curve (use this)  
ggplot(ROC_df) +
  geom_line(aes(x=FPR, y=TPR), size=1) +
  labs(y="TPR", x = "FPR", color=" ")
```
Besides, Figure \@ref(fig:fig5) may help us select $t$ if necessary.
```{r fig5, fig.align='center', fig.cap="TPR/FPR vs. t", echo=FALSE, message=FALSE}
ggplot(ROC_df) +
  geom_line(aes(x=t, y=TPR, color = "TPR"), size=1) +
  geom_line(aes(x=t, y=FPR, color = "FPR"), size=1) +
  labs(y="TPR/FPR", x = "t", color=" ")
```

## Model validation: step 2


```{r fig6, fig.align='center', fig.cap="Number of Bookings with Children for Each Fold", echo=FALSE, warning=FALSE, message=FALSE}
K_folds = 20

hotel_val = hotel_val %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(hotel_val)) %>% sample)

hotel_val_cv = foreach(fold = 1:K_folds, .combine='rbind') %dopar% {
  hotel_val_folds_train = filter(hotel_val, fold_id == fold)
  hotel_val_folds_phat = predict(lasso_selected, hotel_val_folds_train, type = "response")
  c(y=sum(hotel_val_folds_train$children), E_y=sum(hotel_val_folds_phat)%>%round(0))
} %>% as.data.frame()

hotel_val_cv = hotel_val_cv %>%
  arrange(y) %>%
  mutate(fold_id = rep(1:K_folds))

ggplot(data = hotel_val_cv) +
  geom_line(aes(x=fold_id, y=y, color = "Actual # of children"),  size=1) +
  geom_line(aes(x=fold_id, y=E_y, color = "Predicted # of children"), size=1) +
  labs(y="Predicted / Actual Bookings with Children ", x = "Fold", color=" ")+
  geom_point(aes(x=fold_id, y=y)) +
  geom_point(aes(x=fold_id, y=E_y))

```

Figure \@ref(fig:fig6) shows that the difference of each fold for actual data and predicted model is small, which is always smaller than 5. Therefore, our model does well in predicting the total number of bookings with children in each fold.
